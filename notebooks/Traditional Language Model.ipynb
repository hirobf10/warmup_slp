{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sacremoses import MosesTokenizer\n",
    "from collections import deque, defaultdict\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file_path = \"../data/wiki-en-train.word\"\n",
    "test_file_path = \"../data/wiki-en-test.word\"\n",
    "\n",
    "with open(train_file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    train_file = [line.splitlines()[0] for line in f.readlines()]\n",
    "    \n",
    "with open(test_file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    test_file = [line.splitlines()[0] for line in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Ngram:\n",
    "    def __init__(self, n):\n",
    "        if not isinstance(n, int):\n",
    "            raise TypeError\n",
    "        if n < 1:\n",
    "            raise ValueError\n",
    "        self.n = n\n",
    "        self._trained = False\n",
    "        self.counts = None\n",
    "        self.context_counts = None\n",
    "        self.counts_inv = None\n",
    "        self.prob = None\n",
    "\n",
    "        self.mt = MosesTokenizer(lang='en')\n",
    "        \n",
    "    def __call__(self, text: str):\n",
    "        if not self._trained:\n",
    "            print(\"Call train before predicting\")\n",
    "            return\n",
    "        return self.mle_with_entropy(text)\n",
    "        \n",
    "    def train(self, corpus):\n",
    "        \"\"\"\n",
    "        Train Ngram with the corpus.\n",
    "        \"\"\"\n",
    "        counts = defaultdict(lambda: defaultdict(int))\n",
    "        counts_inv = defaultdict(lambda: defaultdict(int))\n",
    "        \n",
    "        for line in corpus:\n",
    "            tokens = deque(self.mt.tokenize(line))\n",
    "            if self.n == 1:\n",
    "                tokens.appendleft(\"<s>\")\n",
    "                tokens.append(\"</s>\")\n",
    "            else:\n",
    "                for _ in range(self.n-1):\n",
    "                    tokens.appendleft(\"<s>\")\n",
    "                    tokens.append(\"</s>\")\n",
    "            tokens = list(tokens)\n",
    "            for i in range(len(tokens)-self.n+1):\n",
    "                counts[\" \".join(tokens[i:i+self.n-1])][tokens[i:i+self.n][-1]] += 1\n",
    "                counts_inv[tokens[i:i+self.n][-1]][\" \".join(tokens[i:i+self.n-1])] += 1\n",
    "            self.counts = counts\n",
    "            self.counts_inv = counts_inv\n",
    "            \n",
    "        self.prob = self.kneser_ney_smoothing()\n",
    "        self._trained = True\n",
    "    \n",
    "    def kneser_ney_smoothing(self, d=0.75):\n",
    "        prob = defaultdict(lambda: defaultdict(int))\n",
    "        num_ngram_types = len([context for context_count in self.counts_inv.values() for context in context_count.keys()])\n",
    "        for context, word_count in self.counts.items():\n",
    "            context_count = sum([count for count in word_count.values()])\n",
    "            num_context = sum([count for count in word_count.values()])\n",
    "            for word, count in word_count.items():\n",
    "                lmd = d * len(word_count) / num_context\n",
    "                p_cont = len(self.counts_inv[word]) / num_ngram_types\n",
    "                prob[context][word] = max(count-d, 0) / context_count + lmd * p_cont\n",
    "        return prob\n",
    "\n",
    "    def mle(self, text: str, return_tokens=False) -> float:\n",
    "        prob = 0\n",
    "        tokens = deque(self.mt.tokenize(text))\n",
    "        if self.n == 1:\n",
    "            tokens.appendleft(\"<s>\")\n",
    "            tokens.append(\"</s>\")\n",
    "        else:\n",
    "            for _ in range(self.n-1):\n",
    "                tokens.appendleft(\"<s>\")\n",
    "                tokens.append(\"</s>\")\n",
    "        tokens = list(tokens)\n",
    "        for i in range(len(tokens)-self.n+1):\n",
    "            p = self.prob[\" \".join(tokens[i:i+self.n-1])][tokens[i:i+self.n][-1]]\n",
    "            if p == 0:\n",
    "                if return_tokens:\n",
    "                    return 0, tokens\n",
    "                else:\n",
    "                    return 0\n",
    "            else:\n",
    "                prob += math.log(p)\n",
    "        if return_tokens:\n",
    "            return math.exp(prob), tokens\n",
    "        else:\n",
    "            return math.exp(prob)\n",
    "    \n",
    "    def entropy(self, prob, N) -> float:\n",
    "        return - math.log(prob) / N\n",
    "    \n",
    "    def mle_with_entropy(self, text: str) -> float:\n",
    "        prob = 0\n",
    "        tokens = deque(self.mt.tokenize(text))\n",
    "        if self.n == 1:\n",
    "            tokens.appendleft(\"<s>\")\n",
    "            tokens.append(\"</s>\")\n",
    "        else:\n",
    "            for _ in range(self.n-1):\n",
    "                tokens.appendleft(\"<s>\")\n",
    "                tokens.append(\"</s>\")\n",
    "        tokens = list(tokens)\n",
    "        for i in range(len(tokens)-self.n+1):\n",
    "            p = self.prob[\" \".join(tokens[i:i+self.n-1])][tokens[i:i+self.n][-1]]\n",
    "            if p == 0:\n",
    "                continue\n",
    "            else:\n",
    "                prob += math.log(p)\n",
    "        return - prob / len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean of entropy when 2-gram: 1.3907674549577471\n",
      "The mean of entropy when 3-gram: 0.3258129953501907\n",
      "The mean of entropy when 5-gram: 0.16083540294889437\n"
     ]
    }
   ],
   "source": [
    "for n in [2, 3, 5]:\n",
    "    ngram = Ngram(n)\n",
    "    ngram.train(train_file)\n",
    "    entropy_list = [ngram(line) for line in test_file]\n",
    "    print(f\"The mean of entropy when {n}-gram:\", sum(entropy_list) / len(entropy_list))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
